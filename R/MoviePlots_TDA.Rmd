---
title: "MoviePlots_TDA"
author: "Matt O'Reilly"
date: "3/25/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Import libraries
library(plyr) #  for pre-processing 
library(tidyverse) # for pre-processing and visualisation
library(readxl)

#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(superml)


library(TDAmapper) #Mapper algorithm
library(igraph) #Plotting mapper
library(TDAstats)
library(TDA)
library(class)
```


```{r}
movies <- read_excel("../data/mpst_full_data.xlsx", col_types = 'text', .name_repair = "minimal")
movies <- movies [,1:6]

#Subset genres we will study
movies_single_genre <- movies[which(sapply(strsplit(movies$tags, " "), length) == 1),]
movies_romantic <- movies_single_genre[movies_single_genre$tags == 'romantic',]
movies_romantic <- movies_romantic[1:250,]
movies_entertainment <- movies_single_genre[movies_single_genre$tags == 'entertaining',]
movies_entertainment <- movies_entertainment[1:250,]
movies_comedy <- movies_single_genre[movies_single_genre$tags == 'comedy',]
movies_comedy <- movies_comedy[1:250,]
movies_murder <- movies_single_genre[movies_single_genre$tags == 'murder',]
movies_murder <- movies_murder[1:250,]


movies <- movies[1:800,]
movies<- movies[which(sapply(strsplit(movies$tags, " "), length) > 1),]
```

#Testing Conditions.
```{r}
subset <- grepl("romantic", movies$tags) | grepl("entertaining", movies$tags) | grepl("comedy", movies$tags) | grepl("murder", movies$tags)
#Subset Data so we have at least one of our chosen 4 categories to identify
movies<- movies[subset,]


movie_tags <- Corpus(VectorSource(movies$tags))

pattern = c("boring", "adult", "bleak", "cult", "flashback", "humor", "satire","paranormal","suspenseful", "good versus evil", "historical fiction", "psychedelic", "violence", "historical fiction", "allegory", "fantasy","horror", "gothic", "atmospheric", "inspiring", "queer", "stupid", "feel-good", "cruelty", "dramatic","action", "revenge", "sadist", "mystery", "neo noir", "tragedy", "haunting", "sentimental" , "historical", "storytelling", "home movie", "philosophical", "cute", "pornographic", "plot twist", "prank", "claustrophobic", "insanity", "brainwashing", "sci-fi", "dark", "psychological", "absurd", "alternate reality", "alternate history", "comic", "grindhouse film", "thought-provoking", "melodrama", "depressing", "realism", "western", "clever", "whimsical", "ent...", "intrigue", "realism", "anti war", "blaxploitation", "avant garde", "autobiographical", "suicidal")

corpus <- tm_map(movie_tags, removeWords, pattern)
movies$tags <- lapply(corpus, as.character)
movies$tags <- gsub("[.?!]*(?=[.?!]$)", "", movies$tags, perl=T)
#movies$tags <- gsub(",","", movies$tags, perl=T)

movies$tags
```


```{r}
clean_text <- function(x){ 
  gsub("…|⋆|–|‹|”|“|‘|’", " ", x) 
}

removeSingle <- function(x) gsub(" . ", " ", x)   

preprocess_corpus <- function(corpus){
  # Convert the text to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  # Remove punctuations
  corpus <- tm_map(corpus, removePunctuation)
  # Remove special characters from text
  corpus <- tm_map(corpus, clean_text)
  # Remove english common stopwords
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  # Remove name of newspapers from the corpus
  corpus <- tm_map(corpus, removeWords, c("eagle rising","freedom daily"))
  # 'stem' words to root words
  corpus <- tm_map(corpus,stemDocument)
  # Eliminate extra white spaces
  corpus <- tm_map(corpus, stripWhitespace)
  #Remove single letter words
  corpus <- tm_map(corpus, content_transformer(removeSingle))
  terms <-TermDocumentMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))

  return (terms)
}

```

```{r, warning=FALSE}
barcode <- function(x) {
  category_tdm <- preprocess_corpus(Corpus(VectorSource(x)))
  category_tdm <- removeSparseTerms(category_tdm, 0.93)
  hom <- calculate_homology(category_tdm)
  plot_barcode(hom)
}
barcode(movies_comedy$plot_synopsis)
barcode(movies_entertainment$plot_synopsis)
barcode(movies_murder$plot_synopsis)
barcode(movies_romantic$plot_synopsis)
```

```{r warning=FALSE, include=FALSE}
bettinum <- function(x){
article <- as.character(x) 
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
#article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.005) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
View(movies)
```

#KNN Clustering Algorithm with BettiNum input
```{r}
set.seed(8)
mat.df <- as.data.frame(as.matrix(dist(movies$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, movies$tags)
colnames(mat.df)[ncol(mat.df)] <- "category"

train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]

cl <- mat.df[,"category"]

modeldata <- mat.df[,!colnames(mat.df) %in% "category"]
knn.pred <- knn(modeldata[train,], modeldata[test, ], cl[train], k = sqrt(nrow(text_data)))
conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```