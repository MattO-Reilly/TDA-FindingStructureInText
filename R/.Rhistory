V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
buzzfeed_real <- buzzfeed_df[1:91,]
mean_real <- sum(unlist(buzzfeed_real$Betti_Num))/nrow(buzzfeed_real)
sd_real <- sd(unlist(buzzfeed_real$Betti_Num))
error_real <- qnorm(0.95)*sd_real/sqrt(nrow(buzzfeed_real))
mean_real
#Fake articles
buzzfeed_fake <-  buzzfeed_df[92:180,]
mean_fake <- sum(unlist(buzzfeed_fake$Betti_Num))/nrow(buzzfeed_fake)
sd_fake <- sd(unlist(buzzfeed_fake$Betti_Num))
error_fake <- qnorm(0.95)*sd_fake/sqrt(nrow(buzzfeed_fake))
mean_fake
mat.df <- as.data.frame(as.matrix(dist(buzzfeed_df$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, buzzfeed_df$type)
colnames(mat.df)[ncol(mat.df)] <- "category"
train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]
cl <- mat.df[,"category"]
modeldata <- mat.df[,!colnames(mat.df) %in% "category"]
knn.pred <- knn(modeldata[train,], modeldata[test, ], cl[train], k = sqrt(nrow(buzzfeed_df)))
conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy <- sum(diag(conf.mat))/length(test) * 100)
# Precision: tp/(tp+fp):
precision <- conf.mat[1,1]/sum(conf.mat[1,1:2])
# Recall: tp/(tp + fn):
recall <- conf.mat[1,1]/sum(conf.mat[1:2,1])
# F-Score: 2 * precision * recall /(precision + recall):
Fscore <- 2 * precision * recall / (precision + recall)
Fscore
bettinum <- function(x){
article <- as.character(x)
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.0005) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
distance_matrix = bettiNum.dist,
filter_values = 1:nrow(movies),
num_intervals = 4,
percent_overlap = 50,
num_bins_when_clustering = 65)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
points.in.vertex <- m1$points_in_vertex[[i]]
vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
library(mldr)
library(utiml)
library(randomForest)
mat.df <- as.data.frame(as.matrix(dist(movies$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, movies[,7:10])
# Create two partitions (train and test) of toyml multi-label dataset
movies_mldr <- mldr_from_dataframe(mat.df, labelIndices = c(455:458), attributes, "movies")
# Create two partitions (train, test) of emotions dataset
partitions <- c(train = 0.7, test = 0.3)
ds <- create_holdout_partition(movies_mldr, partitions, method="iterative")
#MLKNN
model <- mlknn(movies_mldr, k = 4, s=1)
pred <- predict(model, movies_mldr)
# Predict
test <- predict(model, ds$test, cores=parallel::detectCores())
#test
# Evaluate the models
measures <- c("F1")
result <- cbind(
Test = multilabel_evaluate(ds$tes, test, measures)
)
print(round(result, 3))
bettinum <- function(x){
article <- as.character(x)
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
#article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.0005) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
distance_matrix = bettiNum.dist,
filter_values = 1:nrow(movies),
num_intervals = 4,
percent_overlap = 50,
num_bins_when_clustering = 65)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
points.in.vertex <- m1$points_in_vertex[[i]]
vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
library(mldr)
library(utiml)
library(randomForest)
mat.df <- as.data.frame(as.matrix(dist(movies$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, movies[,7:10])
# Create two partitions (train and test) of toyml multi-label dataset
movies_mldr <- mldr_from_dataframe(mat.df, labelIndices = c(455:458), attributes, "movies")
# Create two partitions (train, test) of emotions dataset
partitions <- c(train = 0.7, test = 0.3)
ds <- create_holdout_partition(movies_mldr, partitions, method="iterative")
#MLKNN
model <- mlknn(movies_mldr, k = 4, s=1)
pred <- predict(model, movies_mldr)
# Predict
test <- predict(model, ds$test, cores=parallel::detectCores())
#test
# Evaluate the models
measures <- c("F1")
result <- cbind(
Test = multilabel_evaluate(ds$tes, test, measures)
)
print(round(result, 3))
bettinum <- function(x){
article <- as.character(x)
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
#article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.0001) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
distance_matrix = bettiNum.dist,
filter_values = 1:nrow(movies),
num_intervals = 4,
percent_overlap = 50,
num_bins_when_clustering = 65)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
points.in.vertex <- m1$points_in_vertex[[i]]
vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
library(mldr)
library(utiml)
library(randomForest)
mat.df <- as.data.frame(as.matrix(dist(movies$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, movies[,7:10])
# Create two partitions (train and test) of toyml multi-label dataset
movies_mldr <- mldr_from_dataframe(mat.df, labelIndices = c(455:458), attributes, "movies")
# Create two partitions (train, test) of emotions dataset
partitions <- c(train = 0.7, test = 0.3)
ds <- create_holdout_partition(movies_mldr, partitions, method="iterative")
#MLKNN
model <- mlknn(movies_mldr, k = 4, s=1)
pred <- predict(model, movies_mldr)
# Predict
test <- predict(model, ds$test, cores=parallel::detectCores())
#test
# Evaluate the models
measures <- c("F1")
result <- cbind(
Test = multilabel_evaluate(ds$tes, test, measures)
)
print(round(result, 3))
bettinum <- function(x){
article <- as.character(x)
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
#article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.001) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
bettinum <- function(x){
article <- as.character(x)
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
#article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.001) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
distance_matrix = bettiNum.dist,
filter_values = 1:nrow(movies),
num_intervals = 4,
percent_overlap = 50,
num_bins_when_clustering = 65)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
points.in.vertex <- m1$points_in_vertex[[i]]
vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
library(mldr)
library(utiml)
library(randomForest)
mat.df <- as.data.frame(as.matrix(dist(movies$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, movies[,7:10])
# Create two partitions (train and test) of toyml multi-label dataset
movies_mldr <- mldr_from_dataframe(mat.df, labelIndices = c(455:458), attributes, "movies")
# Create two partitions (train, test) of emotions dataset
partitions <- c(train = 0.7, test = 0.3)
ds <- create_holdout_partition(movies_mldr, partitions, method="iterative")
#MLKNN
model <- mlknn(movies_mldr, k = 4, s=1)
pred <- predict(model, movies_mldr)
# Predict
test <- predict(model, ds$test, cores=parallel::detectCores())
#test
# Evaluate the models
measures <- c("F1")
result <- cbind(
Test = multilabel_evaluate(ds$tes, test, measures)
)
print(round(result, 3))
bettinum <- function(x){
article <- as.character(x)
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
#article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.005) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
distance_matrix = bettiNum.dist,
filter_values = 1:nrow(movies),
num_intervals = 4,
percent_overlap = 50,
num_bins_when_clustering = 65)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
points.in.vertex <- m1$points_in_vertex[[i]]
vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
library(mldr)
library(utiml)
library(randomForest)
mat.df <- as.data.frame(as.matrix(dist(movies$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, movies[,7:10])
# Create two partitions (train and test) of toyml multi-label dataset
movies_mldr <- mldr_from_dataframe(mat.df, labelIndices = c(455:458), attributes, "movies")
# Create two partitions (train, test) of emotions dataset
partitions <- c(train = 0.7, test = 0.3)
ds <- create_holdout_partition(movies_mldr, partitions, method="iterative")
#MLKNN
model <- mlknn(movies_mldr, k = 4, s=1)
pred <- predict(model, movies_mldr)
# Predict
test <- predict(model, ds$test, cores=parallel::detectCores())
#test
# Evaluate the models
measures <- c("F1")
result <- cbind(
Test = multilabel_evaluate(ds$tes, test, measures)
)
print(round(result, 3))
bettinum <- function(x){
article <- as.character(x)
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
#article_tdm <- removeSparseTerms(article_tdm, 0.95)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.0025) & hom[, "dimension"] == 1)
}
#movies$plot_synopsis[1:5]
bettiNum_list <- lapply(movies$plot_synopsis, bettinum)
movies$Betti_Num <- bettiNum_list
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
distance_matrix = bettiNum.dist,
filter_values = 1:nrow(movies),
num_intervals = 4,
percent_overlap = 50,
num_bins_when_clustering = 65)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
points.in.vertex <- m1$points_in_vertex[[i]]
vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
library(mldr)
library(utiml)
library(randomForest)
mat.df <- as.data.frame(as.matrix(dist(movies$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, movies[,7:10])
# Create two partitions (train and test) of toyml multi-label dataset
movies_mldr <- mldr_from_dataframe(mat.df, labelIndices = c(455:458), attributes, "movies")
# Create two partitions (train, test) of emotions dataset
partitions <- c(train = 0.7, test = 0.3)
ds <- create_holdout_partition(movies_mldr, partitions, method="iterative")
#MLKNN
model <- mlknn(movies_mldr, k = 4, s=1)
pred <- predict(model, movies_mldr)
# Predict
test <- predict(model, ds$test, cores=parallel::detectCores())
#test
# Evaluate the models
measures <- c("F1")
result <- cbind(
Test = multilabel_evaluate(ds$tes, test, measures)
)
print(round(result, 3))
knitr::opts_chunk$set(echo = TRUE)
# Import libraries
library(plyr) #  for pre-processing
library(tidyverse) # for pre-processing and visualisation
library(readxl)
library(igraph)
#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(class)
library(TDAstats)
library(TDA)
library(TDAmapper) #Mapper algorithm
buzzfeed_real <- read_excel('../data/Buzzfeed/BuzzFeed_real_news_content.xlsx', col_types = "text")
buzzfeed_fake <- read_excel('../data/Buzzfeed/BuzzFeed_fake_news_content.xlsx', col_types = "text")
# merge data frames and delete old data frames
buzzfeed_df = rbind(buzzfeed_real, buzzfeed_fake)
# adding new column of type for categorising document as real or fake
buzzfeed_df$type <- sapply(strsplit(buzzfeed_df$id, "_"), head,  1)
# check the dimensions of the datset
dim(buzzfeed_df)
# check the summary of dataset
summary(buzzfeed_df)
# select necessary columns from the dataframe for analysis
buzzfeed_df <- buzzfeed_df[c("id","title","text","type")]
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove website error articles
buzzfeed_real <- buzzfeed_df[1:91,]
buzzfeed_fake <- buzzfeed_df[92:180, ]
irrelevant_words <- c("american", "campaign", "can", "clinton", "clinton‚äô", "countri", "debat", "donald", "don‚äôt", "email", "even", "first", "foundat", "get", "hillari", "just", "know", "like", "make", "mani", "nation", "need", "new", "news", "obama", "one", "peopl", "presid", "report", "right", "said", "say", "state", "take", "thing", "think", "time" , "trump", "will", "‚äì" , "‚äî", "‚äôs", "also", "call", "now", "share", "elect", "muslim", "terrorist", "want", "polic", "adult", "also", "attack", "author", "can", "candid", "clinton", "countri", "day", "donald", "first", "get", "hillari", "just", "know", "last", "like", "live", "make", "may", "need", "new", "news", "now", "peopl", "polic", "polit", "presid", "question", "said", "say", "septemb", "support", "thing", "time", "trump", "two", "want", "week", "world",  "year", "york", "‚äôs", "‚äù", "‚äúi", "american", "ask", "black", "call", "continu", "even", "offic", "one", "presidenti", "protest", "report", "republican", "right", "see", "show", "think", "will", "work", "‚äî", "nation", "obama", "state", "take", "war", "debat", "democrat", "elect", "monday", "poll", "stori", "trump‚äô", "way", "former", "campaign", "polici", "told", "page")
clean_text <- function(x){
gsub("…|⋆|–|‹|”|“|‘|’", " ", x)
}
removeSingle <- function(x) gsub(" . ", " ", x)
preprocess_corpus <- function(corpus){
# Convert the text to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove numbers
corpus <- tm_map(corpus, removeNumbers)
# Remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# Remove special characters from text
corpus <- tm_map(corpus, clean_text)
# Remove english common stopwords
#corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Remove name of newspapers from the corpus
#corpus <- tm_map(corpus, removeWords, irrelevant_words)
# 'stem' words to root words
corpus <- tm_map(corpus,stemDocument)
# Eliminate extra white spaces
corpus <- tm_map(corpus, stripWhitespace)
#Remove single letter words
corpus <- tm_map(corpus, content_transformer(removeSingle))
terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))
return (terms)
}
#Text processing
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove website error articles
#buzzfeed_df$text <- gsub('\n', '[.]', buzzfeed_df$text)
buzzfeed_df$text <- gsub("[^\u0001-\u007F]+|<U\\+\\w+>","", buzzfeed_df$text) #remove non ascii characters
#Create Corpus of all articles and apply Natural Language Process to it
article_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_df$text)))
set.seed(10)
#Create Corpus of all articles and apply Natural Language Process to it
Fake_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_fake$text)))
Fake_lf_terms <- findFreqTerms(Fake_tdm, lowfreq=50)
Fake_tdm <- removeSparseTerms(Fake_tdm , 0.73)
Fake_barcode <- ripsDiag(as.matrix(Fake_tdm), 1, 0.15, printProgress = FALSE)
plot(Fake_barcode[["diagram"]],barcode = TRUE)
Fake_freqwords <- findMostFreqTerms(Fake_tdm, n = 100)
#Create Corpus of all articles and apply Natural Language Process to it
Real_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_real$text)))
Real_tdm <- removeSparseTerms(Real_tdm , 0.73)
Real_barcode <- ripsDiag(as.matrix(Real_tdm), 1, 0.15, printProgress = FALSE)
plot(Real_barcode[["diagram"]],barcode = TRUE)
Real_freqwords <- findMostFreqTerms(Real_tdm, n = 100)
bettinum <- function(x){
article <- as.character(x)
article_tdm <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_tdm)))
#article_tdm <- removeSparseTerms(article_tdm, 0.93)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.0005) & hom[, "dimension"] == 1)
}
bettiNum_list <- as.list(apply(buzzfeed_df['text'],1 , bettinum))
buzzfeed_df$Betti_Num <- bettiNum_list
#kmeans <- kmeans(bettiNum_list, 2, nstart = 5 ,iter.max = 15)
#kmeans
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
distance_matrix = bettiNum.dist,
filter_values = 1:nrow(buzzfeed_df),
num_intervals = 2,
percent_overlap = 50,
num_bins_when_clustering = 40)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
points.in.vertex <- m1$points_in_vertex[[i]]
vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size
plot(m1.graph,main ="Mapper Graph")
buzzfeed_real <- buzzfeed_df[1:91,]
mean_real <- sum(unlist(buzzfeed_real$Betti_Num))/nrow(buzzfeed_real)
sd_real <- sd(unlist(buzzfeed_real$Betti_Num))
error_real <- qnorm(0.95)*sd_real/sqrt(nrow(buzzfeed_real))
mean_real
#Fake articles
buzzfeed_fake <-  buzzfeed_df[92:180,]
mean_fake <- sum(unlist(buzzfeed_fake$Betti_Num))/nrow(buzzfeed_fake)
sd_fake <- sd(unlist(buzzfeed_fake$Betti_Num))
error_fake <- qnorm(0.95)*sd_fake/sqrt(nrow(buzzfeed_fake))
mean_fake
mat.df <- as.data.frame(as.matrix(dist(buzzfeed_df$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, buzzfeed_df$type)
colnames(mat.df)[ncol(mat.df)] <- "category"
train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]
cl <- mat.df[,"category"]
modeldata <- mat.df[,!colnames(mat.df) %in% "category"]
knn.pred <- knn(modeldata[train,], modeldata[test, ], cl[train], k = sqrt(nrow(buzzfeed_df)))
conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy <- sum(diag(conf.mat))/length(test) * 100)
# Precision: tp/(tp+fp):
precision <- conf.mat[1,1]/sum(conf.mat[1,1:2])
# Recall: tp/(tp + fn):
recall <- conf.mat[1,1]/sum(conf.mat[1:2,1])
# F-Score: 2 * precision * recall /(precision + recall):
Fscore <- 2 * precision * recall / (precision + recall)
Fscore
View(bettiNum_list)
View(bettiNum_list)
bettinum <- function(x){
article <- as.character(x)
article_tdm <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_tdm)))
#article_tdm <- removeSparseTerms(article_tdm, 0.93)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.00001) & hom[, "dimension"] == 1)
}
bettiNum_list <- as.list(apply(buzzfeed_df['text'],1 , bettinum))
buzzfeed_df$Betti_Num <- bettiNum_list
#kmeans <- kmeans(bettiNum_list, 2, nstart = 5 ,iter.max = 15)
#kmeans
View(bettiNum_list)
View(bettiNum_list)
buzzfeed_real <- buzzfeed_df[1:91,]
mean_real <- sum(unlist(buzzfeed_real$Betti_Num))/nrow(buzzfeed_real)
sd_real <- sd(unlist(buzzfeed_real$Betti_Num))
error_real <- qnorm(0.95)*sd_real/sqrt(nrow(buzzfeed_real))
mean_real
#Fake articles
buzzfeed_fake <-  buzzfeed_df[92:180,]
mean_fake <- sum(unlist(buzzfeed_fake$Betti_Num))/nrow(buzzfeed_fake)
sd_fake <- sd(unlist(buzzfeed_fake$Betti_Num))
error_fake <- qnorm(0.95)*sd_fake/sqrt(nrow(buzzfeed_fake))
mean_fake
