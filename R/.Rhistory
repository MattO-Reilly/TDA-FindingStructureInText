classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=2, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
tfv <- TfIdfVectorizer$new(max_features = 100, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 100, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
Real_articles <- buzzfeed_df[1:91,]
tail(Real_articles)
Fake_articles <- buzzfeed_df[91:180,]
tail(Fake_articles)
Average_Real_bettinum <- mean(as.numeric(Real_articles$Betti_Num))
Average_Real_bettinum #4.5
Average_Fake_bettinum <- mean(as.numeric(Fake_articles$Betti_Num))
Average_Fake_bettinum #2.5
wfv <- CountVectorizer$new(max_features = 1000, remove_stopwords = TRUE)
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 1000, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
Real_articles <- buzzfeed_df[1:91,]
tail(Real_articles)
Fake_articles <- buzzfeed_df[91:180,]
tail(Fake_articles)
Average_Real_bettinum <- mean(as.numeric(Real_articles$Betti_Num))
Average_Real_bettinum #4.5
Average_Fake_bettinum <- mean(as.numeric(Fake_articles$Betti_Num))
Average_Fake_bettinum #2.5
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 5, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
Real_articles <- buzzfeed_df[1:91,]
tail(Real_articles)
Fake_articles <- buzzfeed_df[91:180,]
tail(Fake_articles)
Average_Real_bettinum <- mean(as.numeric(Real_articles$Betti_Num))
Average_Real_bettinum #4.5
Average_Fake_bettinum <- mean(as.numeric(Fake_articles$Betti_Num))
Average_Fake_bettinum #2.5
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 5, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
cf_mat2 <- tfv$fit_transform(article)
cf_mat1 + cf_mat2
hom <- calculate_homology(cf_mat1, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 5, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
cf_mat2 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
hom1 <- calculate_homology(cf_mat2, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 5, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
cf_mat2 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
hom1 <- calculate_homology(cf_mat2, return_df = TRUE)
hom + hom1
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
as.data.frame(cf_mat1 + cf_mat2)
as.data.frame(cf_mat1) + as.data.frame(cf_mat2)
merge(cf_mat1, cf_mat2, all=TRUE)
wordcountfreq[is.na(wordcountfreq)] <- 0
wordcountfreq <- merge(cf_mat1, cf_mat2, all=TRUE)
wordcountfreq[is.na(wordcountfreq)] <- 0
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 5, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
cf_mat2 <- tfv$fit_transform(article)
wordcountfreq <- merge(cf_mat1, cf_mat2, all=TRUE)
wordcountfreq[is.na(wordcountfreq)] <- 0
#hom <- calculate_homology(cf_mat1, return_df = TRUE)
hom <- calculate_homology(wordcountfreq, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:5){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)qq
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:5){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
cf_mat2 <- tfv$fit_transform(article)
wordcountfreq <- merge(cf_mat1, cf_mat2, all=TRUE)
wordcountfreq[is.na(wordcountfreq)] <- 0
#hom <- kcalculate_homology(cf_mat1, return_df = TRUE)
hom <- calculate_homology(wordcountfreq, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:5){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
cf_mat2 <- tfv$fit_transform(article)
wordcountfreq <- merge(cf_mat1, cf_mat2, all=TRUE)
wordcountfreq[is.na(wordcountfreq)] <- 0
#hom <- kcalculate_homology(cf_mat1, return_df = TRUE)
hom <- calculate_homology(wordcountfreq, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
#buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
View(bettiNum_list)
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <-lemmatize_strings(article)
cf_mat1 <- wfv$fit_transform(article)
cf_mat2 <- tfv$fit_transform(article)
wordcountfreq <- merge(cf_mat1, cf_mat2, all=TRUE)
wordcountfreq[is.na(wordcountfreq)] <- 0
#hom <- kcalculate_homology(cf_mat1, return_df = TRUE)
hom <- calculate_homology(wordcountfreq, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
View(bettiNum_list)
Real_articles <- buzzfeed_df[1:91,]
tail(Real_articles)
Fake_articles <- buzzfeed_df[91:180,]
tail(Fake_articles)
Average_Real_bettinum <- mean(as.numeric(Real_articles$Betti_Num))
Average_Real_bettinum #4.5
Average_Fake_bettinum <- mean(as.numeric(Fake_articles$Betti_Num))
Average_Fake_bettinum #2.5
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=4, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=5, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=6, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=3, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=4, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=10, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=9, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=8, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=7, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=6, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
for (i in 1:nrow(buzzfeed_df)){
classification <- as.list(ifelse(buzzfeed_df$Betti_Num >=7, 'Real', 'Fake'))
}
buzzfeed_df$classification <- classification
correct <- sum(ifelse(buzzfeed_df$type == buzzfeed_df$classification, 1, 0)) #Num of correctly classified news articles.
correct/count(buzzfeed_df)*100
??vectorizer
??bottleneck distance
??bottleneck
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <- clean_text(article)
article <-lemmatize_strings(article)
cf_mat1 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
View(bettiNum_list)
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <- clean_text(article)
article <-lemmatize_strings(article)
cf_mat1 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.1) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
View(bettiNum_list)
View(bettiNum_list)
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(buzzfeed_df)){
article <- unlist(buzzfeed_df$text[i], use.names=FALSE)
article <- clean_text(article)
article <-lemmatize_strings(article)
cf_mat1 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.05) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
buzzfeed_df$Betti_Num <- bettiNum_list
#Fake_news_data$ID <- rep('Fake', num_articles)
View(bettiNum_list)
article <- unlist(buzzfeed_df$text[27], use.names=FALSE)
article <- clean_text(article)
article <-lemmatize_strings(article)
cf_mat1 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = TRUE)
hom <- calculate_homology(cf_mat1, return_df = FALSE)
plot_barcode(hom)
# Buzzfeed title corpus
buzzfeed_df$text <- tokenize_sentences(buzzfeed_df$text, lowercase = TRUE, strip_punct = TRUE, simplify = FALSE)
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove error articles
#lemmatize_strings(buzzfeed_df$text)
article_corpus <- Corpus(VectorSource(buzzfeed_df$text))
# convert title corpus to document term matrix
article_corpus <- preprocess_corpus(article_corpus)
#article_corpus$text_processed[1]
article_df <- data.frame(text_processed=sapply(article_corpus, identity),
stringsAsFactors=F)
View(article_df)
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(article_df)){
article <- unlist(article_df$text_processed[i], use.names=FALSE)
article <- clean_text(article)
article <-lemmatize_strings(article)
cf_mat1 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = FALSE)
plot_barcode(hom)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.05) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
View(article_df)
knitr::opts_chunk$set(echo = TRUE)
# Import libraries
library(plyr) #  for pre-processing
library(tidyverse) # for pre-processing and visualisation
library(readxl)
#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(TDAstats)
buzzfeed_real <- read_excel('../data/Buzzfeed/BuzzFeed_real_news_content.xlsx', col_types = "text")
buzzfeed_fake <- read_excel('../data/Buzzfeed/BuzzFeed_fake_news_content.xlsx', col_types = "text")
# merge data frames and delete old data frames
buzzfeed_df = rbind(buzzfeed_real, buzzfeed_fake)
# adding new column of type for categorising document as real or fake
buzzfeed_df$type <- sapply(strsplit(buzzfeed_df$id, "_"), head,  1)
# check the dimensions of the datset
dim(buzzfeed_df)
# check the summary of dataset
summary(buzzfeed_df)
# select necessary columns from the dataframe for analysis
buzzfeed_df <- buzzfeed_df[c("id","title","text","type")]
clean_text <- function(x){
gsub("…|⋆|–|‹|”|“|‘|’", " ", x)
}
preprocess_corpus <- function(corpus){
# Convert the text to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove numbers
corpus <- tm_map(corpus, removeNumbers)
# Remove punctuations
# corpus <- tm_map(corpus, removePunctuation)
# Remove special characters from text
corpus <- tm_map(corpus, clean_text)
# Remove english common stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Remove name of newspapers from the corpus
corpus <- tm_map(corpus, removeWords, c("eagle rising","freedom daily"))
# 'stem' words to root words
corpus <- tm_map(corpus,stemDocument)
# Eliminate extra white spaces
corpus <- tm_map(corpus, stripWhitespace)
article_df <- data.frame(text_processed=sapply(corpus, identity),
stringsAsFactors=F)
return (article_df)
}
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove error articles
#lemmatize_strings(buzzfeed_df$text)
article_corpus <- Corpus(VectorSource(buzzfeed_df$text))
# convert title corpus to document term matrix
article_corpus <- preprocess_corpus(article_corpus)
#article_corpus$text_processed[1]
article_df <- data.frame(text_processed=sapply(article_corpus, identity),
stringsAsFactors=F)
buzzfeed_df$text <- tokenize_sentences(buzzfeed_df$text, lowercase = TRUE, strip_punct = TRUE, simplify = FALSE)
#buzzfeed_df <- cbind(buzzfeed_df,article_corpus)
#buzzfeed_df$text_processed <- tokenize_sentences(as.String(buzzfeed_df$text_processed))
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(article_df)){
article <- unlist(article_df$text_processed[i], use.names=FALSE)
article <- clean_text(article)
article <-lemmatize_strings(article)
cf_mat1 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = FALSE)
plot_barcode(hom)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.05) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
View(article_df)
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove error articles
#lemmatize_strings(buzzfeed_df$text)
article_corpus <- Corpus(VectorSource(buzzfeed_df$text))
# convert title corpus to document term matrix
article_corpus <- preprocess_corpus(article_corpus)
#article_corpus$text_processed[1]
article_df <- data.frame(text_processed=sapply(article_corpus, identity),
stringsAsFactors=F)
article_df$text <- tokenize_sentences(article_df$text_processed, lowercase = TRUE, strip_punct = TRUE, simplify = FALSE)
buzzfeed_df$text <- tokenize_sentences(buzzfeed_df$text, lowercase = TRUE, strip_punct = TRUE, simplify = FALSE)
View(article_df)
knitr::opts_chunk$set(echo = TRUE)
# Import libraries
library(plyr) #  for pre-processing
library(tidyverse) # for pre-processing and visualisation
library(readxl)
#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(TDAstats)
buzzfeed_real <- read_excel('../data/Buzzfeed/BuzzFeed_real_news_content.xlsx', col_types = "text")
buzzfeed_fake <- read_excel('../data/Buzzfeed/BuzzFeed_fake_news_content.xlsx', col_types = "text")
# merge data frames and delete old data frames
buzzfeed_df = rbind(buzzfeed_real, buzzfeed_fake)
# adding new column of type for categorising document as real or fake
buzzfeed_df$type <- sapply(strsplit(buzzfeed_df$id, "_"), head,  1)
# check the dimensions of the datset
dim(buzzfeed_df)
# check the summary of dataset
summary(buzzfeed_df)
# select necessary columns from the dataframe for analysis
buzzfeed_df <- buzzfeed_df[c("id","title","text","type")]
clean_text <- function(x){
gsub("…|⋆|–|‹|”|“|‘|’", " ", x)
}
preprocess_corpus <- function(corpus){
# Convert the text to lower case
corpus <- tm_map(corpus, content_transformer(tolower))
# Remove numbers
corpus <- tm_map(corpus, removeNumbers)
# Remove punctuations
# corpus <- tm_map(corpus, removePunctuation)
# Remove special characters from text
corpus <- tm_map(corpus, clean_text)
# Remove english common stopwords
corpus <- tm_map(corpus, removeWords, stopwords("english"))
# Remove name of newspapers from the corpus
corpus <- tm_map(corpus, removeWords, c("eagle rising","freedom daily"))
# 'stem' words to root words
corpus <- tm_map(corpus,stemDocument)
# Eliminate extra white spaces
corpus <- tm_map(corpus, stripWhitespace)
article_df <- data.frame(text_processed=sapply(corpus, identity),
stringsAsFactors=F)
return (article_df)
}
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove error articles
#lemmatize_strings(buzzfeed_df$text)
article_corpus <- Corpus(VectorSource(buzzfeed_df$text))
# convert title corpus to document term matrix
article_corpus <- preprocess_corpus(article_corpus)
#article_corpus$text_processed[1]
article_df <- data.frame(text_processed=sapply(article_corpus, identity),
stringsAsFactors=F)
article_df$text <- tokenize_sentences(article_df$text_processed, lowercase = TRUE, strip_punct = TRUE, simplify = FALSE)
buzzfeed_df$text <- tokenize_sentences(buzzfeed_df$text, lowercase = TRUE, strip_punct = TRUE, simplify = FALSE)
#buzzfeed_df <- cbind(buzzfeed_df,article_corpus)
#buzzfeed_df$text_processed <- tokenize_sentences(as.String(buzzfeed_df$text_processed))
tfv <- TfIdfVectorizer$new(max_features = 15, remove_stopwords = TRUE)
wfv <- CountVectorizer$new(max_features = 15, remove_stopwords = TRUE)
bettiNum_list <- list()
for (i in 1:nrow(article_df)){
article <- unlist(article_df$text_processed[i], use.names=FALSE)
article <- clean_text(article)
article <-lemmatize_strings(article)
cf_mat1 <- tfv$fit_transform(article)
hom <- calculate_homology(cf_mat1, return_df = FALSE)
plot_barcode(hom)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.05) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
}
View(article_df)
View(article_df)
buzzfeed_df$text[5]
buzzfeed_real <- read_excel('../data/Buzzfeed/BuzzFeed_real_news_content.xlsx', col_types = "text")
buzzfeed_fake <- read_excel('../data/Buzzfeed/BuzzFeed_fake_news_content.xlsx', col_types = "text")
buzzfeed_real$text[5]
readlines(buzzfeed_real$text[5])
read_lines(buzzfeed_real$text[5])
readline(buzzfeed_real$text[5])
