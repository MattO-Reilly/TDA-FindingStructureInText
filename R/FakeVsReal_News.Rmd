---
title: "FakeNewsAnalysis"
author: "Matt O'Reilly"
date: "2/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Import libraries
library(plyr) #  for pre-processing 
library(tidyverse) # for pre-processing and visualisation
library(readxl)
library(igraph)

#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer

library(TDAstats)
library(TDA)
library(TDAmapper) #Mapper algorithm

```

```{r}
buzzfeed_real <- read_excel('../data/Buzzfeed/BuzzFeed_real_news_content.xlsx', col_types = "text")
buzzfeed_fake <- read_excel('../data/Buzzfeed/BuzzFeed_fake_news_content.xlsx', col_types = "text")
```


```{r}
# merge data frames and delete old data frames 
buzzfeed_df = rbind(buzzfeed_real, buzzfeed_fake)

# adding new column of type for categorising document as real or fake 
buzzfeed_df$type <- sapply(strsplit(buzzfeed_df$id, "_"), head,  1)
```

```{r}
# check the dimensions of the datset
dim(buzzfeed_df)

# check the summary of dataset
summary(buzzfeed_df)
```


```{r}
# select necessary columns from the dataframe for analysis
buzzfeed_df <- buzzfeed_df[c("id","title","text","type")]
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove website error articles 

buzzfeed_real <- buzzfeed_df[1:91,]
buzzfeed_fake <- buzzfeed_df[92:180, ]
```

```{r}
irrelevant_words <- c("american", "campaign", "can", "clinton", "clinton‚äô", "countri", "debat", "donald", "don‚äôt", "email", "even", "first", "foundat", "get", "hillari", "just", "know", "like", "make", "mani", "nation", "need", "new", "news", "obama", "one", "peopl", "presid", "report", "right", "said", "say", "state", "take", "thing", "think", "time" , "trump", "will", "‚äì" , "‚äî", "‚äôs", "also", "call", "now", "share", "elect", "muslim", "terrorist", "want", "polic", "adult", "also", "attack", "author", "can", "candid", "clinton", "countri", "day", "donald", "first", "get", "hillari", "just", "know", "last", "like", "live", "make", "may", "need", "new", "news", "now", "peopl", "polic", "polit", "presid", "question", "said", "say", "septemb", "support", "thing", "time", "trump", "two", "want", "week", "world",  "year", "york", "‚äôs", "‚äù", "‚äúi", "american", "ask", "black", "call", "continu", "even", "offic", "one", "presidenti", "protest", "report", "republican", "right", "see", "show", "think", "will", "work", "‚äî", "nation", "obama", "state", "take", "war", "debat", "democrat", "elect", "monday", "poll", "stori", "trump‚äô", "way", "former", "campaign", "polici", "told", "page")

clean_text <- function(x){ 
  gsub("…|⋆|–|‹|”|“|‘|’", " ", x) 
}

removeSingle <- function(x) gsub(" . ", " ", x)   

preprocess_corpus <- function(corpus){
  # Convert the text to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  # Remove punctuations
  corpus <- tm_map(corpus, removePunctuation)
  # Remove special characters from text
  corpus <- tm_map(corpus, clean_text)
  # Remove english common stopwords
  #corpus <- tm_map(corpus, removeWords, stopwords("english"))
  # Remove name of newspapers from the corpus
  #corpus <- tm_map(corpus, removeWords, irrelevant_words)
  # 'stem' words to root words
  corpus <- tm_map(corpus,stemDocument)
  # Eliminate extra white spaces
  corpus <- tm_map(corpus, stripWhitespace)
  #Remove single letter words
  corpus <- tm_map(corpus, content_transformer(removeSingle))
  terms <-TermDocumentMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))

  return (terms)
}
```


###Article Title Analysis
```{r}
#Text processing
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove website error articles 
#buzzfeed_df$text <- gsub('\n', '[.]', buzzfeed_df$text)
buzzfeed_df$text <- gsub("[^\u0001-\u007F]+|<U\\+\\w+>","", buzzfeed_df$text) #remove non ascii characters



#Create Corpus of all articles and apply Natural Language Process to it
article_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_df$text)))
```

#Fake Articles Barcode
```{r}
#Create Corpus of all articles and apply Natural Language Process to it
Fake_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_fake$text)))
Fake_lf_terms <- findFreqTerms(Fake_tdm, lowfreq=50) 
Fake_tdm <- removeSparseTerms(Fake_tdm , 0.73)
Fake_barcode <- ripsDiag(as.matrix(Fake_tdm), 1, 0.2, printProgress = FALSE)
plot(Fake_barcode[["diagram"]],barcode = TRUE)
Fake_freqwords <- findMostFreqTerms(Fake_tdm, n = 100)
```

#Real Articles Barcode
```{r}
#Create Corpus of all articles and apply Natural Language Process to it
Real_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_real$text)))
Real_tdm <- removeSparseTerms(Real_tdm , 0.73)
Real_barcode <- ripsDiag(as.matrix(Real_tdm), 1, 0.2, printProgress = FALSE)
plot(Real_barcode[["diagram"]],barcode = TRUE)
Real_freqwords <- findMostFreqTerms(Real_tdm, n = 100)
```

```{r warning=FALSE, include=FALSE}
bettinum <- function(x){
article <- as.character(x) 
article_tdm <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_tdm)))
article_tdm <- removeSparseTerms(article_tdm, 0.93)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.001) & hom[, "dimension"] == 1)
}

bettiNum_list <- as.list(apply(buzzfeed_df['text'],1 , bettinum))
buzzfeed_df$Betti_Num <- bettiNum_list

#kmeans <- kmeans(bettiNum_list, 2, nstart = 5 ,iter.max = 15)
#kmeans
```

```{r}
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
    distance_matrix = bettiNum.dist,
    filter_values = 1:nrow(buzzfeed_df),
    num_intervals = 2,
    percent_overlap = 50,
    num_bins_when_clustering = 10)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )


vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
  points.in.vertex <- m1$points_in_vertex[[i]]
  vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size

plot(m1.graph,main ="Mapper Graph")
```

```{r}
buzzfeed_df$Category_TDA <- NA
buzzfeed_df$correct <- NA

buzzfeed_df$Category_TDA[buzzfeed_df$Betti_Num <= 4] = 'Real'
buzzfeed_df$Category_TDA[buzzfeed_df$Betti_Num == 5 ] = 'Real / Fake'
buzzfeed_df$Category_TDA[buzzfeed_df$Betti_Num >= 6] = 'Fake'

value <- stringr::str_detect( buzzfeed_df$Category_TDA,buzzfeed_df$type)
buzzfeed_df$correct <- value
buzzfeed_df$correct <- as.integer(as.logical(buzzfeed_df$correct))
buzzfeed_df$correct[is.na(buzzfeed_df$correct)]<- 0
sum(buzzfeed_df$correct/nrow(buzzfeed_df))
```


```{r}
buzzfeed_real <- buzzfeed_df[1:91,]
mean_real <- sum(unlist(buzzfeed_real$Betti_Num))/nrow(buzzfeed_real)
sd_real <- sd(unlist(buzzfeed_real$Betti_Num))
error_real <- qnorm(0.95)*sd_real/sqrt(nrow(buzzfeed_real))
mean_real

#Fake articles
buzzfeed_fake <-  buzzfeed_df[92:180,]
mean_fake <- sum(unlist(buzzfeed_fake$Betti_Num))/nrow(buzzfeed_fake)
sd_fake <- sd(unlist(buzzfeed_fake$Betti_Num))
error_fake <- qnorm(0.95)*sd_fake/sqrt(nrow(buzzfeed_fake))
mean_fake
```
The 95% confidence interval for Fake articles is
`r round((mean_fake - error_fake),2)`, `r round((mean_fake + error_fake),2)`
(2.7,4.2)


The 95% confidence interval for Real articles is
`r round((mean_real - error_real),2)`, `r round((mean_real + error_real),2)`
(2.28,3.32)

