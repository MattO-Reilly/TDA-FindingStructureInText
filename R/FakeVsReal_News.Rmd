---
title: "FakeNewsAnalysis"
author: "Matt O'Reilly"
date: "2/12/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Import libraries
library(plyr) #  for pre-processing 
library(tidyverse) # for pre-processing and visualisation
library(readxl)
library(igraph)

#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(class)


library(TDAstats)
library(TDA)
library(TDAmapper) #Mapper algorithm

```

```{r}
buzzfeed_real <- read_excel('./data/Buzzfeed/BuzzFeed_real_news_content.xlsx', col_types = "text")
buzzfeed_fake <- read_excel('./data/Buzzfeed/BuzzFeed_fake_news_content.xlsx', col_types = "text")
```


```{r}
# merge data frames and delete old data frames 
buzzfeed_df = rbind(buzzfeed_real, buzzfeed_fake)

# adding new column of type for categorising document as real or fake 
buzzfeed_df$type <- sapply(strsplit(buzzfeed_df$id, "_"), head,  1)
```

```{r}
# check the dimensions of the datset
dim(buzzfeed_df)

# check the summary of dataset
summary(buzzfeed_df)
```


```{r}
# select necessary columns from the dataframe for analysis
buzzfeed_df <- buzzfeed_df[c("id","title","text","type")]
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove website error articles 

buzzfeed_real <- buzzfeed_df[1:91,]
buzzfeed_fake <- buzzfeed_df[92:180, ]
```

```{r}
irrelevant_words <- c("american", "campaign", "can", "clinton", "clinton‚äô", "countri", "debat", "donald", "don‚äôt", "email", "even", "first", "foundat", "get", "hillari", "just", "know", "like", "make", "mani", "nation", "need", "new", "news", "obama", "one", "peopl", "presid", "report", "right", "said", "say", "state", "take", "thing", "think", "time" , "trump", "will", "‚äì" , "‚äî", "‚äôs", "also", "call", "now", "share", "elect", "muslim", "terrorist", "want", "polic", "adult", "also", "attack", "author", "can", "candid", "clinton", "countri", "day", "donald", "first", "get", "hillari", "just", "know", "last", "like", "live", "make", "may", "need", "new", "news", "now", "peopl", "polic", "polit", "presid", "question", "said", "say", "septemb", "support", "thing", "time", "trump", "two", "want", "week", "world",  "year", "york", "‚äôs", "‚äù", "‚äúi", "american", "ask", "black", "call", "continu", "even", "offic", "one", "presidenti", "protest", "report", "republican", "right", "see", "show", "think", "will", "work", "‚äî", "nation", "obama", "state", "take", "war", "debat", "democrat", "elect", "monday", "poll", "stori", "trump‚äô", "way", "former", "campaign", "polici", "told", "page")

clean_text <- function(x){ 
  gsub("…|⋆|–|‹|”|“|‘|’", " ", x) 
}

removeSingle <- function(x) gsub(" . ", " ", x)   

preprocess_corpus <- function(corpus){
  # Convert the text to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  # Remove punctuations
  corpus <- tm_map(corpus, removePunctuation)
  # Remove special characters from text
  corpus <- tm_map(corpus, clean_text)
  # Remove english common stopwords
  #corpus <- tm_map(corpus, removeWords, stopwords("english"))
  # Remove name of newspapers from the corpus
  #corpus <- tm_map(corpus, removeWords, irrelevant_words)
  # 'stem' words to root words
  corpus <- tm_map(corpus,stemDocument)
  # Eliminate extra white spaces
  corpus <- tm_map(corpus, stripWhitespace)
  #Remove single letter words
  corpus <- tm_map(corpus, content_transformer(removeSingle))
  terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))

  return (terms)
}
```


###Article Text Analysis
```{r}
#Text processing
buzzfeed_df <- buzzfeed_df[-c(96, 124), ]  #Remove website error articles 
#buzzfeed_df$text <- gsub('\n', '[.]', buzzfeed_df$text)
buzzfeed_df$text <- gsub("[^\u0001-\u007F]+|<U\\+\\w+>","", buzzfeed_df$text) #remove non ascii characters



#Create Corpus of all articles and apply Natural Language Process to it
article_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_df$text)))
```

#Fake Articles Barcode
```{r}

#Create Corpus of all articles and apply Natural Language Process to it
Fake_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_fake$text)))
Fake_lf_terms <- findFreqTerms(Fake_tdm, lowfreq=50) 
Fake_tdm <- removeSparseTerms(Fake_tdm , 0.73)
Fake_barcode <- ripsDiag(as.matrix(Fake_tdm), 1, 0.15, printProgress = FALSE)
plot(Fake_barcode[["diagram"]],barcode = TRUE)
Fake_freqwords <- findMostFreqTerms(Fake_tdm, n = 100)
```

#Real Articles Barcode
```{r}
#Create Corpus of all articles and apply Natural Language Process to it
Real_tdm <- preprocess_corpus(Corpus(VectorSource(buzzfeed_real$text)))
Real_tdm <- removeSparseTerms(Real_tdm , 0.73)
Real_barcode <- ripsDiag(as.matrix(Real_tdm), 1, 0.15, printProgress = FALSE)
plot(Real_barcode[["diagram"]],barcode = TRUE)
Real_freqwords <- findMostFreqTerms(Real_tdm, n = 100)
```

```{r warning=FALSE, include=FALSE}
bettinum <- function(x){
article <- as.character(x) 
article_tdm <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_tdm)))
#article_tdm <- removeSparseTerms(article_tdm, 0.93)
hom <- calculate_homology(article_tdm) 
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.00001) & hom[, "dimension"] == 1)
}

bettiNum_list <- as.list(apply(buzzfeed_df['text'],1 , bettinum))
buzzfeed_df$Betti_Num <- bettiNum_list

#kmeans <- kmeans(bettiNum_list, 2, nstart = 5 ,iter.max = 15)
#kmeans
```





```{r}
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum_list)
m1 <- mapper1D(
    distance_matrix = bettiNum.dist,
    filter_values = 1:nrow(buzzfeed_df),
    num_intervals = 2,
    percent_overlap = 50,
    num_bins_when_clustering = 20)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )
```


```{r}
buzzfeed_real <- buzzfeed_df[1:91,]
mean_real <- sum(unlist(buzzfeed_real$Betti_Num))/nrow(buzzfeed_real)
sd_real <- sd(unlist(buzzfeed_real$Betti_Num))
error_real <- qnorm(0.95)*sd_real/sqrt(nrow(buzzfeed_real))
mean_real

#Fake articles
buzzfeed_fake <-  buzzfeed_df[92:180,]
mean_fake <- sum(unlist(buzzfeed_fake$Betti_Num))/nrow(buzzfeed_fake)
sd_fake <- sd(unlist(buzzfeed_fake$Betti_Num))
error_fake <- qnorm(0.95)*sd_fake/sqrt(nrow(buzzfeed_fake))
mean_fake
```
The 95% confidence interval for Fake articles is
`r round((mean_fake - error_fake),2)`, `r round((mean_fake + error_fake),2)`
(2.7,4.2)


The 95% confidence interval for Real articles is
`r round((mean_real - error_real),2)`, `r round((mean_real + error_real),2)`
(2.28,3.32)


#KNN Clustering Algorithm with BettiNum input
```{r}
mat.df <- as.data.frame(as.matrix(dist(buzzfeed_df$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, buzzfeed_df$type)
colnames(mat.df)[ncol(mat.df)] <- "category"

train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]
cl <- mat.df[,"category"]

modeldata <- mat.df[,!colnames(mat.df) %in% "category"]
knn.pred <- knn(modeldata[train,], modeldata[test, ], cl[train], k = sqrt(nrow(buzzfeed_df)))
conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy <- sum(diag(conf.mat))/length(test) * 100)

# Precision: tp/(tp+fp):
precision <- conf.mat[1,1]/sum(conf.mat[1,1:2])

# Recall: tp/(tp + fn):
recall <- conf.mat[1,1]/sum(conf.mat[1:2,1])

# F-Score: 2 * precision * recall /(precision + recall):
Fscore <- 2 * precision * recall / (precision + recall)

Fscore
```

