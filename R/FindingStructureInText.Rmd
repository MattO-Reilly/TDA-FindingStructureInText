---
title: "Text_TDA"
author: "Matt O'Reilly"
date: "1/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Import libraries
library(plyr) #  for pre-processing 
library(tidyverse) # for pre-processing and visualisation
library(readxl)

#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(superml)


library(TDAmapper) #Mapper algorithm
library(igraph) #Plotting mapper
library(TDAstats)
library(TDA)
library(class)
```

```{r}
num_articles = 200
text_data <- read_excel("../data/freq_vectors.xlsx", col_types = 'text', .name_repair = "minimal")
text_data <- text_data[1:num_articles,]
```

```{r}
text_data_tech <- text_data[text_data$category == 'tech', ]
text_data_tech

text_data_business <- text_data[text_data$category == 'business', ]
text_data_business

text_data_sport <- text_data[text_data$category == 'sport', ]
text_data_sport

text_data_entertainment <- text_data[text_data$category == 'entertainment', ]
text_data_entertainment

text_data_politics <- text_data[text_data$category == 'politics', ]
text_data_politics

```

```{r}
clean_text <- function(x){ 
  gsub("…|⋆|–|‹|”|“|‘|’", " ", x) 
}

removeSingle <- function(x) gsub(" . ", " ", x)   

preprocess_corpus <- function(corpus){
  # Convert the text to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  # Remove punctuations
  corpus <- tm_map(corpus, removePunctuation)
  # Remove special characters from text
  corpus <- tm_map(corpus, clean_text)
  # Remove english common stopwords
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  # Remove name of newspapers from the corpus
  corpus <- tm_map(corpus, removeWords, c("eagle rising","freedom daily"))
  # 'stem' words to root words
  corpus <- tm_map(corpus,stemDocument)
  # Eliminate extra white spaces
  corpus <- tm_map(corpus, stripWhitespace)
  #Remove single letter words
  corpus <- tm_map(corpus, content_transformer(removeSingle))
  terms <-DocumentTermMatrix(corpus,control = list(weighting = function(x) weightTfIdf(x, normalize = TRUE)))

  return (terms)
}

```

```{r warning=FALSE}
tech_tdm <- preprocess_corpus(Corpus(VectorSource(text_data_tech$text)))
business_tdm <- preprocess_corpus(Corpus(VectorSource(text_data_business$text)))
sport_tdm <- preprocess_corpus(Corpus(VectorSource(text_data_sport$text)))
entertainment_tdm <- preprocess_corpus(Corpus(VectorSource(text_data_entertainment$text)))
politics_tdm <- preprocess_corpus(Corpus(VectorSource(text_data_politics$text)))
```

```{r}
as.matrix(tech_tdm)
tech_tdm <- removeSparseTerms(tech_tdm, 0.6)
tech_barcode <- ripsDiag(as.matrix(tech_tdm), 1, 0.2, printProgress = FALSE)
tech_barcode
plot(tech_barcode[["diagram"]],barcode = TRUE)
tech_freqwords <- findMostFreqTerms(tech_tdm, n = 100)
```

```{r}
business_tdm <- removeSparseTerms(business_tdm, 0.6)
business_barcode <- ripsDiag(as.matrix(business_tdm), 1, 0.2, printProgress = FALSE)
business_barcode
plot(business_barcode[["diagram"]],barcode = TRUE)
business_freqwords <- findMostFreqTerms(business_tdm, n = 100)
```

```{r}
sport_tdm <- removeSparseTerms(sport_tdm, 0.6)
sport_barcode <- ripsDiag(as.matrix(sport_tdm), 1, 0.2, printProgress = FALSE)
sport_barcode
plot(sport_barcode[["diagram"]],barcode = TRUE)
sport_freqwords <- findMostFreqTerms(sport_tdm, n = 100)
```

```{r}
entertainment_tdm <- removeSparseTerms(entertainment_tdm, 0.6)
entertainment_barcode <- ripsDiag(as.matrix(entertainment_tdm), 1, 0.2, printProgress = FALSE)
entertainment_barcode
plot(entertainment_barcode[["diagram"]],barcode = TRUE)
entertainment_barcode[['diagram']]
entertainment_freqwords <- findMostFreqTerms(entertainment_tdm, n = 100)
```

```{r}
politics_tdm <- removeSparseTerms(politics_tdm, 0.6)
politics_barcode <- ripsDiag(as.matrix(politics_tdm), 1, 0.2, printProgress = FALSE)
politics_barcode
plot(politics_barcode[["diagram"]],barcode = TRUE)
politics_freqwords <- findMostFreqTerms(politics_tdm, n = 100)
```

```{r warning=FALSE, include=FALSE}
bettinum <- function(x){
article <- as.character(x) 
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
article_tdm <- preprocess_corpus(Corpus(VectorSource(article_lines)))
article_tdm <- removeSparseTerms(article_tdm, 0.9)
hom <- calculate_homology(article_tdm)
bettinum <- sum((hom[, "death"] - hom[, "birth"] >=0.005) & hom[, "dimension"] == 1)
}

bettiNum_list <- apply(text_data['text'],1 , bettinum)
text_data$Betti_Num <- bettiNum_list
View(text_data)

#kmeans <- kmeans(bettiNum_list, 5, nstart = 5 ,iter.max = 15)
#View(text_data)
```

```{r eval=FALSE, include=FALSE}
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum)
m1 <- mapper1D(
    distance_matrix = bettiNum.dist,
    filter_values = 1:num_articles,
    num_intervals = 5,
    percent_overlap = 50,
    num_bins_when_clustering = 10)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")

vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
  points.in.vertex <- m1$points_in_vertex[[i]]
  vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size

plot(m1.graph,main ="Mapper Graph")
legend(x=-2, y=-1, c("y small","y medium","large y"),pch=21,
       col="#777777", pt.bg=c(1,0.5,0), pt.cex=2, cex=.8, bty="n", ncol=1)
```

#KNN Clustering Algorithm with TF-idf input
```{r}
set.seed(9)
article_tdm <- preprocess_corpus(Corpus(VectorSource(text_data$text)))
mat.df <- as.data.frame(data.matrix(article_tdm), stringsAsfactors = FALSE)
mat.df <- cbind(mat.df, text_data$category)
colnames(mat.df)[ncol(mat.df)] <- "category"

train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]
cl <- mat.df$category

modeldata <- mat.df[,!colnames(mat.df) %in% "category"]
knn.pred <- knn(modeldata[train,], modeldata[test, ], cl[train], k = sqrt(num_articles))
conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```
#KNN Clustering Algorithm with BettiNum input
```{r}
set.seed(8)
mat.df <- as.data.frame(as.matrix(dist(text_data$Betti_Num), stringsAsfactors = FALSE))
mat.df <- cbind(mat.df, text_data$category)
colnames(mat.df)[ncol(mat.df)] <- "category"
train <- sample(nrow(mat.df), ceiling(nrow(mat.df) * .70))
test <- (1:nrow(mat.df))[- train]
cl <- mat.df[,"category"]

modeldata <- mat.df[,!colnames(mat.df) %in% "category"]
knn.pred <- knn(modeldata[train,], modeldata[test, ], cl[train], k = sqrt(nrow(text_data)))
conf.mat <- table("Predictions" = knn.pred, Actual = cl[test])
conf.mat
(accuracy <- sum(diag(conf.mat))/length(test) * 100)
```