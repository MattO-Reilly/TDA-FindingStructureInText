---
title: "Text_TDA"
author: "Matt O'Reilly"
date: "1/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Import libraries
library(plyr) #  for pre-processing 
library(tidyverse) # for pre-processing and visualisation
library(readxl)

#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(superml)
library(TDAmapper) #Mapper algorithm
library(igraph) #Plotting mapper



library(TDAstats)
library(TDA)
library(nonlinearTseries)
```

```{r}
num_articles = 50
text_data <- read_excel("../data/freq_vectors.xlsx", col_types = 'text', .name_repair = "minimal")
text_data <- text_data[1:num_articles,]
```

```{r}
text_data_tech <- text_data[text_data$category == 'tech', ]
text_data_tech

text_data_business <- text_data[text_data$category == 'business', ]
text_data_business

text_data_sport <- text_data[text_data$category == 'sport', ]
text_data_sport

text_data_entertainment <- text_data[text_data$category == 'entertainment', ]
text_data_entertainment

text_data_politics <- text_data[text_data$category == 'politics', ]
text_data_politics

```

```{r}
clean_text <- function(x){ 
  gsub("…|⋆|–|‹|”|“|‘|’", " ", x) 
}

removeSingle <- function(x) gsub(" . ", " ", x)   

preprocess_corpus <- function(corpus){
  # Convert the text to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  # Remove punctuations
  corpus <- tm_map(corpus, removePunctuation)
  # Remove special characters from text
  corpus <- tm_map(corpus, clean_text)
  # Remove english common stopwords
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  # Remove name of newspapers from the corpus
  corpus <- tm_map(corpus, removeWords, c("eagle rising","freedom daily"))
  # 'stem' words to root words
  corpus <- tm_map(corpus,stemDocument)
  # Eliminate extra white spaces
  corpus <- tm_map(corpus, stripWhitespace)
  #Remove single letter words
  corpus <- tm_map(corpus, content_transformer(removeSingle))
  return (corpus)
}

```

```{r warning=FALSE}
article_tech <- preprocess_corpus(Corpus(VectorSource(text_data_tech$text)))
article_business <- preprocess_corpus(Corpus(VectorSource(text_data_business$text)))
article_sport <- preprocess_corpus(Corpus(VectorSource(text_data_sport$text)))
article_entertainment <- preprocess_corpus(Corpus(VectorSource(text_data_entertainment$text)))
article_politics <- preprocess_corpus(Corpus(VectorSource(text_data_politics$text)))
```

```{r}
tech_dtm <- TermDocumentMatrix(article_tech)
tech_dtm <- removeSparseTerms(tech_dtm, 0.6)
tech_barcode <- ripsDiag(as.matrix(tech_dtm), 1, 5, printProgress = FALSE)
plot(tech_barcode[["diagram"]],barcode = TRUE)
tech_freqwords <- findMostFreqTerms(tech_dtm, n = 100)
```

```{r}
business_dtm <- TermDocumentMatrix(article_business)
business_dtm <- removeSparseTerms(business_dtm, 0.60)
business_barcode <- ripsDiag(as.matrix(business_dtm), 1, 5, printProgress = FALSE)
plot(business_barcode[["diagram"]],barcode = TRUE)
business_freqwords <- findMostFreqTerms(business_dtm, n = 100)
```

```{r}
sport_dtm <- TermDocumentMatrix(article_sport)
sport_dtm <- removeSparseTerms(sport_dtm, 0.60)
sport_barcode <- ripsDiag(as.matrix(sport_dtm), 1, 5, printProgress = FALSE)
plot(sport_barcode[["diagram"]],barcode = TRUE)
sport_freqwords <- findMostFreqTerms(sport_dtm, n = 100)
```

```{r}
entertainment_dtm <- TermDocumentMatrix(article_entertainment)
entertainment_dtm <- removeSparseTerms(entertainment_dtm, 0.60)
entertainment_barcode <- ripsDiag(as.matrix(entertainment_dtm), 1, 5, printProgress = FALSE)
plot(entertainment_barcode[["diagram"]],barcode = TRUE)
entertainment_freqwords <- findMostFreqTerms(entertainment_dtm, n = 100)
```

```{r}
politics_dtm <- TermDocumentMatrix(article_politics)
politics_dtm <- removeSparseTerms(politics_dtm, 0.60)
politics_barcode <- ripsDiag(as.matrix(politics_dtm), 1, 5, printProgress = FALSE)
plot(politics_barcode[["diagram"]],barcode = TRUE)
politics_freqwords <- findMostFreqTerms(politics_dtm, n = 100)
#findFreqTerms(review_dtm, 10)
```
```{r warning=FALSE, include=FALSE}
bettiNum_list <- list()
bettinum <- function(x){
article <- as.character(x) 
article_lines <- unlist(strsplit(article, "(?<=[[:alnum:]]{3})[?!.]\\s", perl=TRUE)) #Split article into sentences
#article_lines
article_lines <- preprocess_corpus(Corpus(VectorSource(article_lines)))
article_tdm <- removeSparseTerms(TermDocumentMatrix(article_lines), 0.90)
inspect(article_tdm)
hom <- calculate_homology(article_tdm)
#plot_barcode((hom))
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1)
#bettiNum_list <- append(bettiNum_list, bettinum)
}

bettiNum_list <- as.list(apply(text_data['text'],1 , bettinum))
text_data$Betti_Num <- bettiNum_list

#kmeans <- kmeans(bettiNum_list, 5, nstart = 5 ,iter.max = 15)
#View(text_data)
```

```{r eval=FALSE, include=FALSE}
bettiNum <- as.matrix(bettiNum_list)
bettiNum.dist <- dist(bettiNum)
m1 <- mapper1D(
    distance_matrix = bettiNum.dist,
    filter_values = 1:num_articles,
    num_intervals = 5,
    percent_overlap = 50,
    num_bins_when_clustering = 10)
m1

vertex.size <- rep(0,m1$num_vertices)
for (i in 1:m1$num_vertices){
  points.in.vertex <- m1$points_in_vertex[[i]]
  vertex.size[i] <- length((m1$points_in_vertex[[i]]))
}
V(m1.graph)$size <- vertex.size

plot(m1.graph,main ="Mapper Graph")
legend(x=-2, y=-1, c("y small","y medium","large y"),pch=21,
       col="#777777", pt.bg=c(1,0.5,0), pt.cex=2, cex=.8, bty="n", ncol=1)
```

```{r}
text_data$Category_TDA <- NA
text_data$correct <- NA

text_data$Category_TDA[text_data$Betti_Num < 2] = 'entertainment'
text_data$Category_TDA[text_data$Betti_Num >= 2 & text_data$Betti_Num <= 3] = 'entertainment / business / sport'
text_data$Category_TDA[text_data$Betti_Num >3 & text_data$Betti_Num <= 4] = 'business / sport'
text_data$Category_TDA[text_data$Betti_Num >4 & text_data$Betti_Num <= 5] = 'sport'
text_data$Category_TDA[text_data$Betti_Num >5 & text_data$Betti_Num <= 6] = 'tech'
text_data$Category_TDA[text_data$Betti_Num >6 & text_data$Betti_Num <= 9] = 'tech / politics'
text_data$Category_TDA[text_data$Betti_Num > 9] = 'politics'


value <- stringr::str_detect( text_data$Category_TDA,text_data$category)
text_data$correct <- value
text_data$correct <- as.integer(as.logical(text_data$correct))
sum(text_data$correct/num_articles)
```

