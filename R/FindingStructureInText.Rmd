---
title: "Text_TDA"
author: "Matt O'Reilly"
date: "1/1/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# Import libraries
library(plyr) #  for pre-processing 
library(tidyverse) # for pre-processing and visualisation
library(readxl)

#Natural Language Processing
library(superml)
library(tokenizers) #tokenize_sentences function
library(qdap)#rm_stopwords function
library(tm) #NLP
library(textstem) #Text Lemmatizer
library(superml)

library(TDAstats)
library(TDA)
library(nonlinearTseries)
```

```{r}
num_articles = 20
text_data <- read_excel("../data/freq_vectors.xlsx", col_types = 'text', .name_repair = "minimal")
text_data <- text_data[1:num_articles,]
```

```{r}
text_data_tech <- text_data[text_data$category == 'tech', ]
text_data_tech

text_data_business <- text_data[text_data$category == 'business', ]
text_data_business

text_data_sport <- text_data[text_data$category == 'sport', ]
text_data_sport

text_data_entertainment <- text_data[text_data$category == 'entertainment', ]
text_data_entertainment

text_data_politics <- text_data[text_data$category == 'politics', ]
text_data_politics

```

```{r}
clean_text <- function(x){ 
  gsub("…|⋆|–|‹|”|“|‘|’", " ", x) 
}

removeSingle <- function(x) gsub(" . ", " ", x)   

preprocess_corpus <- function(corpus){
  # Convert the text to lower case
  corpus <- tm_map(corpus, content_transformer(tolower))
  # Remove numbers
  corpus <- tm_map(corpus, removeNumbers)
  # Remove punctuations
  corpus <- tm_map(corpus, removePunctuation)
  # Remove special characters from text
  corpus <- tm_map(corpus, clean_text)
  # Remove english common stopwords
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  # Remove name of newspapers from the corpus
  corpus <- tm_map(corpus, removeWords, c("eagle rising","freedom daily"))
  # 'stem' words to root words
  corpus <- tm_map(corpus,stemDocument)
  # Eliminate extra white spaces
  corpus <- tm_map(corpus, stripWhitespace)
  #Remove single letter words
  corpus <- tm_map(corpus, content_transformer(removeSingle))
  return (corpus)
}

```

```{r warning=FALSE}
article_tech <- preprocess_corpus(Corpus(VectorSource(text_data_tech$text)))
article_business <- preprocess_corpus(Corpus(VectorSource(text_data_business$text)))
article_sport <- preprocess_corpus(Corpus(VectorSource(text_data_sport$text)))
article_entertainment <- preprocess_corpus(Corpus(VectorSource(text_data_entertainment$text)))
article_politics <- preprocess_corpus(Corpus(VectorSource(text_data_politics$text)))
```

```{r}
tech_dtm <- TermDocumentMatrix(article_tech)
tech_dtm <- removeSparseTerms(tech_dtm, 0.6)
tech_barcode <- ripsDiag(as.matrix(tech_dtm), 1, 5, printProgress = FALSE)
plot(tech_barcode[["diagram"]],barcode = TRUE)
tech_freqwords <- findMostFreqTerms(tech_dtm, n = 100)
```

```{r}
business_dtm <- TermDocumentMatrix(article_business)
business_dtm <- removeSparseTerms(business_dtm, 0.60)
business_barcode <- ripsDiag(as.matrix(business_dtm), 1, 5, printProgress = FALSE)
plot(business_barcode[["diagram"]],barcode = TRUE)
business_freqwords <- findMostFreqTerms(business_dtm, n = 100)
```

```{r}
sport_dtm <- TermDocumentMatrix(article_sport)
sport_dtm <- removeSparseTerms(sport_dtm, 0.60)
sport_barcode <- ripsDiag(as.matrix(sport_dtm), 1, 5, printProgress = FALSE)
plot(sport_barcode[["diagram"]],barcode = TRUE)
sport_freqwords <- findMostFreqTerms(sport_dtm, n = 100)
```

```{r}
entertainment_dtm <- TermDocumentMatrix(article_entertainment)
entertainment_dtm <- removeSparseTerms(entertainment_dtm, 0.60)
entertainment_barcode <- ripsDiag(as.matrix(entertainment_dtm), 1, 5, printProgress = FALSE)
plot(entertainment_barcode[["diagram"]],barcode = TRUE)
entertainment_freqwords <- findMostFreqTerms(entertainment_dtm, n = 100)
```

```{r}
politics_dtm <- TermDocumentMatrix(article_politics)
politics_dtm <- removeSparseTerms(politics_dtm, 0.60)
politics_barcode <- ripsDiag(as.matrix(politics_dtm), 1, 5, printProgress = FALSE)
plot(politics_barcode[["diagram"]],barcode = TRUE)
politics_freqwords <- findMostFreqTerms(politics_dtm, n = 100)
#findFreqTerms(review_dtm, 10)
```
```{r}
article <- as.character(text_data['text'][2,]) #Remove numbers from articles
article_lines <- unlist(strsplit(article, "[.]"))#Split article into sentences
article_lines <- preprocess_corpus(Corpus(VectorSource(article_lines)))
article_lines
article_tdm <- TermDocumentMatrix(article_lines)
article_barcode <- ripsDiag(as.matrix(article_tdm), 1, 10, printProgress = FALSE)
article_barcode
plot(article_barcode[["diagram"]],barcode = TRUE)
```


```{r warning=FALSE}

tf <- TfIdfVectorizer$new(smooth_idf = TRUE, min_df = 0.1)
bettiNum_list = list()
for (i in 1:nrow(text_data)){
article <- lemmatize_strings(removeNumbers(as.character(text_data['text'][4,]))) #Remove numbers from articles
article_lines <- unlist(strsplit(article, "[.]"))#Split article into sentences
tf_matrix <- tf$fit_transform(article_lines)
hom <- calculate_homology(tf_matrix)
bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.005) & hom[, "dimension"] == 1)
bettiNum_list <- append(bettiNum_list, bettinum)
article_barcode <- ripsDiag(as.matrix(tf_matrix), 1, 10, printProgress = FALSE)
techcat <- bottleneck(article_barcode[["diagram"]], tech_barcode[["diagram"]], dimension = 1)
businesscat <- bottleneck(article_barcode[["diagram"]], business_barcode[["diagram"]], dimension = 1)
entertainmentcat <- bottleneck(article_barcode[["diagram"]], entertainment_barcode[["diagram"]], dimension = 1)
politicscat <- bottleneck(article_barcode[["diagram"]], politics_barcode[["diagram"]], dimension = 1)
sportcat <- bottleneck(article_barcode[["diagram"]], sport_barcode[["diagram"]], dimension = 1)

techcat
businesscat
entertainmentcat
politicscat
sportcat

bottleneck_dist <- c(techcat,businesscat,entertainmentcat,politicscat,sportcat)
which(bottleneck_dist == min(bottleneck_dist))

if (techcat < (businesscat & entertainmentcat & politicscat & sportcat)){
  c_list <- append(c_list, "tech")
  } else if (businesscat < (techcat & entertainmentcat & politicscat & sportcat)){
  c_list <- append(c_list, "business")
} else if (entertainmentcat < (techcat & businesscat & politicscat & sportcat)){
  c_list <- append(c_list, "entertainment")
} else if (politicscat < (techcat & businesscat & entertainmentcat & sportcat)){
  c_list <- append(c_list, "politics")
} else {
  c_list <- append(c_list, "sport")}

}

text_data$Betti_Num <- bettiNum_list
View(text_data)
```

```{r warning=FALSE}
text_data$tdacategory = NA

tfv <- TfIdfVectorizer$new(max_features = 100, remove_stopwords = TRUE)
c_list <- list()

for (i in 1:nrow(text_data)){
article <- text_data$text[i]
article_corpus <- preprocess_corpus(Corpus(VectorSource(article)))
article_tdm <- TermDocumentMatrix(article_corpus, control = list(weightTf(article_corpus)))
article_barcode <- ripsDiag(as.matrix(article_tdm), 2, 10, printProgress = FALSE)
techcat <- bottleneck(article_barcode[["diagram"]], tech_barcode[["diagram"]], dimension = 1)
businesscat <- bottleneck(article_barcode[["diagram"]], business_barcode[["diagram"]], dimension = 1)
entertainmentcat <- bottleneck(article_barcode[["diagram"]], entertainment_barcode[["diagram"]], dimension = 1)
politicscat <- bottleneck(article_barcode[["diagram"]], politics_barcode[["diagram"]], dimension = 1)
sportcat <- bottleneck(article_barcode[["diagram"]], sport_barcode[["diagram"]], dimension = 1)

techcat
businesscat
entertainmentcat
politicscat
sportcat

if (techcat < (businesscat & entertainmentcat & politicscat & sportcat)){
  c_list <- append(c_list, "tech")
  print("1")
  } else if (businesscat < (techcat & entertainmentcat & politicscat & sportcat)){
  print("2")
  c_list <- append(c_list, "business")
} else if (entertainmentcat < (techcat & businesscat & politicscat & sportcat)){
  print("3")
  c_list <- append(c_list, "entertainment")
} else if (politicscat < (techcat & businesscat & entertainmentcat & sportcat)){
  print("4")
  c_list <- append(c_list, "politics")
} else {
  print("5")
  c_list <- append(c_list, "sport")}

}

c_list
View(text_data)
```

