---
title: "FakevsReal_News"
author: "Matt O'Reilly"
date: "1/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Data Manipulation and Read_excel
library(tidyverse) 
library(dplyr)
library(readxl)

library(superml) #Word count vectors
library(TDAmapper) #Mapper algorithm
library(TDAstats) #Calculate homology
library(igraph) #Plotting mapper

#Natural Language Processing
library(tokenizers) #tokenize_sentences
#library(qdap)
library(tm) #NLP
library(textstem) #Text Lemmatizer
```

###Load in Real and Fake news data
```{r}
num_articles = 400 #Pick how many fake and real articles are used


Real_news_data <- read_excel("./data/FakeRealNews/True.xlsx", col_types = "text", .name_repair = "minimal")
#text_vector_data <- text_data %>% select(wordfreqvec)
Real_news_data <- Real_news_data[1:num_articles,1:4]
Real_news_data$text <- tokenize_sentences(Real_news_data$text, lowercase = TRUE, strip_punct = FALSE, simplify = FALSE)

Fake_news_data <- read_excel("./data/FakeRealNews/Fake.xlsx", col_types = "text", .name_repair = "minimal")
#text_vector_data <- text_data %>% select(wordfreqvec)
Fake_news_data <- Fake_news_data[1:num_articles,1:4]
Fake_news_data$text <- tokenize_sentences(Fake_news_data$text, lowercase = TRUE, strip_punct = FALSE, simplify = FALSE)
head(Fake_news_data)
```

###Calculate BettiNumbers for Fake news
```{r}
tfv <- TfIdfVectorizer$new(max_features = 10, remove_stopwords = FALSE)

Fake_bettiNum_list <- list()

for (i in 1:nrow(Fake_news_data)){
  article <- unlist(Fake_news_data$text[i], use.names=FALSE)
  cf_mat1 <- tfv$fit_transform(article)
  hom <- calculate_homology(cf_mat1, return_df = TRUE)
  bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1) 
  Fake_bettiNum_list <- append(Fake_bettiNum_list, bettinum)
}
Fake_news_data$Betti_Num <- Fake_bettiNum_list
Fake_news_data$ID <- rep('Fake', num_articles)
```


###Calculate BettiNumbers for Real news
```{r}
Real_bettiNum_list <- list()
for (i in 1:nrow(Real_news_data)){
  article <- unlist(Real_news_data$text[i], use.names=FALSE)
  cf_mat1 <- tfv$fit_transform(article)
  hom <- calculate_homology(cf_mat1, return_df = TRUE)
  bettinum <- sum((hom[, "death"] - hom[, "birth"] > 0.01) & hom[, "dimension"] == 1) 
  Real_bettiNum_list <- append(Real_bettiNum_list, bettinum)
}
Real_news_data$Betti_Num <- Real_bettiNum_list
Real_news_data$ID <- rep('Real', num_articles)
```


```{r}
Average_Real_bettinum <- mean(as.numeric(Real_news_data$Betti_Num))
Average_Real_bettinum #5.265

Average_Fake_bettinum <- mean(as.numeric(Fake_news_data$Betti_Num))
Average_Fake_bettinum #2.735

Total_news_data <- rbind(Real_news_data,Fake_news_data)

```

```{r}
BettiNum_Category <- list()

for (i in 1:nrow(Total_news_data)){
  classification <- as.list(ifelse(Total_news_data$Betti_Num >= 4, 'Real', 'Fake'))
}
Total_news_data$classification <- classification

correct <- sum(ifelse(Total_news_data$ID == Total_news_data$classification, 1, 0)) #Num of correctly classified news articles.
correct/(num_articles*2)
```


```{r}
bettiNum <- as.matrix(Total_news_data$Betti_Num)
bettiNum.dist <- dist(bettiNum)
m1 <- mapper1D(
    distance_matrix = bettiNum.dist,
    filter_values = 1:800,
    num_intervals = 2,
    percent_overlap = 50,
    num_bins_when_clustering = 10)
m1
m1.graph <- graph.adjacency(m1$adjacency, mode="undirected")
plot(m1.graph, layout = layout.auto(m1.graph) )

```


